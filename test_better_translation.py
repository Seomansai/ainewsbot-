#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞
"""

import os
from openai import OpenAI

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

def test_better_translation():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞"""
    
    openrouter_key = os.getenv('OPENROUTER_API_KEY')
    
    if not openrouter_key:
        print("‚ùå OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
        return
    
    client = OpenAI(
        api_key=openrouter_key,
        base_url="https://openrouter.ai/api/v1"
    )
    
    # –ü—Ä–æ–±–ª–µ–º–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–∏–º–µ—Ä–∞
    test_text = """Reinforcing seamless AI at scale

From large language models (LLMs) to reasoning agents, modern AI tools demand unprecedented computational resources. Trillion-parameter models running on-device and swarms of agents collaborating to complete tasks require a new approach to computing to become truly seamless and ubiquitous. First, technical progress in hardware and silicon design is critical for pushing the boundaries..."""
    
    print("üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞...\n")
    
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    improved_prompt = """–¢—ã –æ–ø—ã—Ç–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ —Å—Ñ–µ—Ä—ã IT –∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. 

–ü–†–ê–í–ò–õ–ê –ü–ï–†–ï–í–û–î–ê:
1. –ü–µ—Ä–µ–≤–æ–¥–∏ –ï–°–¢–ï–°–¢–í–ï–ù–ù–û –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
2. –ê–¥–∞–ø—Ç–∏—Ä—É–π –∞–Ω–≥–ª–∏—Ü–∏–∑–º—ã –ø–æ–¥ —Ä—É—Å—Å–∫—É—é —Ä–µ—á—å
3. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (AI, API, GPU, ML, LLM)
4. –ò–∑–±–µ–≥–∞–π –∫–∞–ª–µ–∫ –∏ –±—É–∫–≤–∞–ª—å–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞
5. –ò—Å–ø–æ–ª—å–∑—É–π –∫—Ä–∞—Ç–∫–∏–µ, –ø–æ–Ω—è—Ç–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
6. –ü—Ä–æ–≤–µ—Ä—è–π –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é

–ü–†–ò–ú–ï–†–´:
- "state-of-the-art" ‚Üí "–ø–µ—Ä–µ–¥–æ–≤–æ–π/—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π/–ª—É—á—à–∏–π"
- "seamless AI" ‚Üí "–±–µ—Å—à–æ–≤–Ω—ã–π –ò–ò" ‚Üí "–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ò–ò"
- "computational costs" ‚Üí "–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã"
- "trillion-parameter models" ‚Üí "–º–æ–¥–µ–ª–∏ —Å —Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
- "swarms of agents" ‚Üí "–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–≥–µ–Ω—Ç–æ–≤"
- "ubiquitous" ‚Üí "–ø–æ–≤—Å–µ–º–µ—Å—Ç–Ω—ã–π/—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π"

–ü–µ—Ä–µ–≤–æ–¥–∏ –∫–∞–∫ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª–µ–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π."""

    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    models = [
        ("meta-llama/llama-3.1-8b-instruct:free", "Llama 3.1 (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)"),
        ("openai/gpt-3.5-turbo", "GPT-3.5 Turbo (–ø–ª–∞—Ç–Ω–æ)"),
        ("anthropic/claude-3-haiku", "Claude 3 Haiku (–ø–ª–∞—Ç–Ω–æ)")
    ]
    
    for model, name in models:
        print(f"üéØ {name}")
        print("=" * 60)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": improved_prompt},
                    {"role": "user", "content": f"–ü–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç:\n\n{test_text}"}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            translation = response.choices[0].message.content.strip()
            print(f"‚úÖ –ü–µ—Ä–µ–≤–æ–¥:\n{translation}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            
        print("-" * 60)
        print()

if __name__ == "__main__":
    test_better_translation() 