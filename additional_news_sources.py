# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥

from typing import Dict, List, Optional
import aiohttp
import asyncio
from datetime import datetime
import feedparser
import json
import re
from bs4 import BeautifulSoup

# ===== –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö –ò–°–¢–û–ß–ù–ò–ö–û–í =====
class ExtendedNewsSources:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–± AI"""
    
    def __init__(self):
        self.sources = {
            # ===== –ú–ï–ñ–î–£–ù–ê–†–û–î–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò =====
            "üåç –û—Å–Ω–æ–≤–Ω—ã–µ": {
                "MIT Technology Review AI": "https://www.technologyreview.com/feed/",
                "TechCrunch AI": "https://techcrunch.com/category/artificial-intelligence/feed/",
                "VentureBeat AI": "https://venturebeat.com/ai/feed/",
                "AI News": "https://artificialintelligence-news.com/feed/",
                "The Verge AI": "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
                "Ars Technica AI": "https://feeds.arstechnica.com/arstechnica/technology-lab",
                "WIRED AI": "https://www.wired.com/feed/tag/ai/latest/rss",
                "IEEE Spectrum AI": "https://spectrum.ieee.org/rss/topic/artificial-intelligence"
            },
            
            "üè¢ –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –±–ª–æ–≥–∏": {
                "OpenAI Blog": "https://openai.com/blog/rss.xml",
                "DeepMind Blog": "https://deepmind.com/blog/feed/basic/",
                "Google AI Blog": "https://ai.googleblog.com/feeds/posts/default",
                "Meta AI Blog": "https://ai.facebook.com/blog/feed/",
                "Microsoft AI Blog": "https://blogs.microsoft.com/ai/feed/",
                "NVIDIA AI Blog": "https://blogs.nvidia.com/blog/category/artificial-intelligence/feed/",
                "Anthropic News": "https://www.anthropic.com/news/rss",
                "Hugging Face Blog": "https://huggingface.co/blog/feed.xml"
            },
            
            "üìö –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ": {
                "AI Research": "https://airesearch.com/feed/",
                "Machine Learning Mastery": "https://machinelearningmastery.com/feed/",
                "Towards Data Science": "https://towardsdatascience.com/feed",
                "AI Papers": "https://aipapers.co/feed",
                "Papers with Code": "https://paperswithcode.com/newsletter/rss"
            },
            
            "üíº –ë–∏–∑–Ω–µ—Å –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏": {
                "CB Insights AI": "https://www.cbinsights.com/research/artificial-intelligence/rss",
                "McKinsey AI": "https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/rss.ashx",
                "Forbes AI": "https://www.forbes.com/ai/feed/",
                "Harvard Business Review AI": "https://hbr.org/topic/artificial-intelligence.rss"
            },
            
            # ===== –†–û–°–°–ò–ô–°–ö–ò–ï –ò–°–¢–û–ß–ù–ò–ö–ò =====
            "üá∑üá∫ –†–æ—Å—Å–∏–π—Å–∫–∏–µ": {
                "–•–∞–±—Ä AI": "https://habr.com/ru/rss/hub/artificial_intelligence/",
                "–•–∞–±—Ä ML": "https://habr.com/ru/rss/hub/machine_learning/",
                "–•–∞–±—Ä Data Science": "https://habr.com/ru/rss/hub/data_science/",
                "CNews AI": "https://www.cnews.ru/inc/rss/news_ai.xml",
                "3DNews AI": "https://3dnews.ru/news/ai/rss/",
                "Tproger AI": "https://tproger.ru/tag/ai/feed/",
                "VC.ru AI": "https://vc.ru/tag/ai/rss",
                "Skillfactory Blog": "https://blog.skillfactory.ru/tag/iskusstvennyj-intellekt/feed/"
            },
            
            "üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ": {
                "AI Education": "https://ai-education.ru/feed/",
                "Machine Learning Russia": "https://mlrussia.ru/feed/",
                "Data Science Russia": "https://datasciencerussia.ru/feed/"
            }
        }
    
    def get_all_sources(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –ø–ª–æ—Å–∫–æ–º –≤–∏–¥–µ"""
        all_sources = {}
        for category, sources in self.sources.items():
            all_sources.update(sources)
        return all_sources
    
    def get_sources_by_category(self, category: str) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        return self.sources.get(category, {})

# ===== –£–ú–ù–´–ô –ü–ê–†–°–ï–† –° –ú–ê–®–ò–ù–ù–´–ú –û–ë–£–ß–ï–ù–ò–ï–ú =====
class SmartNewsParser:
    """–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML"""
    
    def __init__(self):
        self.ai_keywords = {
            # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            "high": [
                "artificial intelligence", "machine learning", "deep learning",
                "neural network", "transformer", "gpt", "llm", "ai model",
                "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ—Å–µ—Ç—å",
                "chatgpt", "claude", "gemini", "copilot"
            ],
            # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç  
            "medium": [
                "automation", "computer vision", "nlp", "natural language",
                "—Ä–æ–±–æ—Ç", "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–∞–ª–≥–æ—Ä–∏—Ç–º", "–¥–∞–Ω–Ω—ã–µ", "–∞–Ω–∞–ª–∏—Ç–∏–∫–∞",
                "prediction", "classification", "regression", "clustering"
            ],
            # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            "low": [
                "technology", "innovation", "digital", "smart", "intelligent",
                "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "–∏–Ω–Ω–æ–≤–∞—Ü–∏—è", "—Ü–∏—Ñ—Ä–æ–≤–æ–π", "—É–º–Ω—ã–π"
            ]
        }
        
        # –ò—Å–∫–ª—é—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞ (–Ω–µ AI –Ω–æ–≤–æ—Å—Ç–∏)
        self.exclude_keywords = [
            "crypto", "bitcoin", "blockchain", "web3", "nft",
            "crypto", "–±–∏—Ç–∫–æ–∏–Ω", "–±–ª–æ–∫—á–µ–π–Ω", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
            "game", "gaming", "mobile", "phone", "smartphone"
        ]
    
    def calculate_ai_relevance_score(self, title: str, description: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –∫ AI"""
        text = f"{title} {description}".lower()
        score = 0.0
        
        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã –∑–∞ AI –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in self.ai_keywords["high"]:
            if keyword in text:
                score += 3.0
        
        for keyword in self.ai_keywords["medium"]:
            if keyword in text:
                score += 1.5
        
        for keyword in self.ai_keywords["low"]:
            if keyword in text:
                score += 0.5
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –∏—Å–∫–ª—é—á–∞—é—â–∏–µ —Å–ª–æ–≤–∞
        for keyword in self.exclude_keywords:
            if keyword in text:
                score -= 2.0
        
        return max(0.0, score)
    
    def is_high_quality_news(self, news_item: dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        title = news_item.get('title', '')
        description = news_item.get('description', '')
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(title) < 10 or len(description) < 50:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º
        spam_indicators = ['click here', 'buy now', 'limited time', 'URGENT']
        text = f"{title} {description}".lower()
        
        for indicator in spam_indicators:
            if indicator.lower() in text:
                return False
        
        # AI —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevance_score = self.calculate_ai_relevance_score(title, description)
        return relevance_score >= 2.0
    
    async def enhanced_rss_parsing(self, url: str, source_name: str) -> List[dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as response:
                    content = await response.text()
                    
            feed = feedparser.parse(content)
            news_items = []
            
            for entry in feed.entries[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                try:
                    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                    news_item = {
                        'title': entry.get('title', ''),
                        'description': self._clean_description(entry.get('summary', '')),
                        'link': entry.get('link', ''),
                        'published': self._parse_date(entry.get('published')),
                        'source': source_name,
                        'author': entry.get('author', ''),
                        'tags': self._extract_tags(entry)
                    }
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
                    if self.is_high_quality_news(news_item):
                        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                        news_item['relevance_score'] = self.calculate_ai_relevance_score(
                            news_item['title'], 
                            news_item['description']
                        )
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                        news_item['image_url'] = self._extract_image_url(entry)
                        
                        news_items.append(news_item)
                        
                except Exception as e:
                    continue
            
            return news_items
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {source_name}: {e}")
            return []
    
    def _clean_description(self, description: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è –æ—Ç HTML –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not description:
            return ""
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        soup = BeautifulSoup(description, 'html.parser')
        text = soup.get_text()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(text) > 500:
            text = text[:500] + "..."
        
        return text
    
    def _parse_date(self, date_str: str) -> datetime:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        if not date_str:
            return datetime.now()
        
        try:
            # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤
            formats = [
                '%a, %d %b %Y %H:%M:%S %z',
                '%a, %d %b %Y %H:%M:%S GMT',
                '%Y-%m-%dT%H:%M:%S%z',
                '%Y-%m-%d %H:%M:%S'
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue
            
            return datetime.now()
        except:
            return datetime.now()
    
    def _extract_tags(self, entry) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ –∑–∞–ø–∏—Å–∏"""
        tags = []
        
        if hasattr(entry, 'tags'):
            for tag in entry.tags:
                if hasattr(tag, 'term'):
                    tags.append(tag.term.lower())
        
        return tags[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤
    
    def _extract_image_url(self, entry) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–ª—è
            if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                return entry.media_thumbnail[0]['url']
            
            if hasattr(entry, 'media_content') and entry.media_content:
                return entry.media_content[0]['url']
            
            # –ò—â–µ–º –≤ description
            if hasattr(entry, 'summary'):
                soup = BeautifulSoup(entry.summary, 'html.parser')
                img_tag = soup.find('img')
                if img_tag and img_tag.get('src'):
                    return img_tag['src']
            
            return None
        except:
            return None

# ===== –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò –ò –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò =====
class SocialMediaParser:
    """–ü–∞—Ä—Å–µ—Ä —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è AI –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        self.twitter_accounts = [
            "OpenAI", "DeepMind", "AnthropicAI", "huggingface",
            "GoogleAI", "MetaAI", "nvidia", "Microsoft"
        ]
        
        self.reddit_subreddits = [
            "MachineLearning", "artificial", "deeplearning",
            "ChatGPT", "LocalLLaMA", "singularity"
        ]
    
    async def parse_twitter_rss(self, account: str) -> List[dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Twitter RSS (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–æ—Ä–æ–Ω–Ω–∏–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è RSS –∏–∑ Twitter
        # –ù–∞–ø—Ä–∏–º–µ—Ä: https://rss.app –∏–ª–∏ https://twitrss.me
        rss_url = f"https://twitrss.me/twitter_user_to_rss/?user={account}"
        
        parser = SmartNewsParser()
        return await parser.enhanced_rss_parsing(rss_url, f"Twitter @{account}")
    
    async def parse_reddit_rss(self, subreddit: str) -> List[dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Reddit RSS"""
        rss_url = f"https://www.reddit.com/r/{subreddit}/hot/.rss"
        
        parser = SmartNewsParser()
        return await parser.enhanced_rss_parsing(rss_url, f"Reddit r/{subreddit}")

# ===== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° –í–ù–ï–®–ù–ò–ú–ò API =====
class NewsAggregator:
    """–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö API"""
    
    def __init__(self, newsapi_key: Optional[str] = None):
        self.newsapi_key = newsapi_key
        self.sources = ExtendedNewsSources()
        self.parser = SmartNewsParser()
    
    async def fetch_from_newsapi(self, query: str = "artificial intelligence") -> List[dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ News API"""
        if not self.newsapi_key:
            return []
        
        url = "https://newsapi.org/v2/everything"
        params = {
            'q': query,
            'apiKey': self.newsapi_key,
            'language': 'en',
            'sortBy': 'publishedAt',
            'pageSize': 20
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params) as response:
                    data = await response.json()
                    
                    news_items = []
                    for article in data.get('articles', []):
                        news_item = {
                            'title': article.get('title', ''),
                            'description': article.get('description', ''),
                            'link': article.get('url', ''),
                            'published': datetime.fromisoformat(
                                article.get('publishedAt', '').replace('Z', '+00:00')
                            ),
                            'source': article.get('source', {}).get('name', 'News API'),
                            'image_url': article.get('urlToImage'),
                            'relevance_score': self.parser.calculate_ai_relevance_score(
                                article.get('title', ''),
                                article.get('description', '')
                            )
                        }
                        
                        if self.parser.is_high_quality_news(news_item):
                            news_items.append(news_item)
                    
                    return news_items
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ News API: {e}")
            return []
    
    async def get_all_news(self) -> List[dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_sources = self.sources.get_all_sources()
        tasks = []
        
        # RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for source_name, url in all_sources.items():
            task = self.parser.enhanced_rss_parsing(url, source_name)
            tasks.append(task)
        
        # News API (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.newsapi_key:
            tasks.append(self.fetch_from_newsapi())
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_news = []
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –¥–∞—Ç–µ
        all_news.sort(key=lambda x: (x.get('relevance_score', 0), x.get('published', datetime.min)), reverse=True)
        
        return all_news[:50]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-50 –Ω–æ–≤–æ—Å—Ç–µ–π 